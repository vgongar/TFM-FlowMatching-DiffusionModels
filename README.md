#  Visualizando _Flow Matching_

- __Autor__: V铆ctor Gonz谩lez Garc铆a

![animation](https://github.com/user-attachments/assets/f7bf6da9-1ee9-42fc-9ff0-e244854a4b5a)

<img width="1346" height="763" alt="program" src="https://github.com/user-attachments/assets/7e2be1d2-1124-4ec0-b8bf-5b8bf3e5f795" />

Esta es una peque帽a aplicaci贸n que nos permite visualizar c贸mo se generar datos a trav茅s de _Flow Matching_La aplicaci贸n nos permite escoger entre dos distribuciones "de juguete" y otra opci贸n en la que podemos dibujar nuestro dataset con el rat贸n. Una vez escogidos los datos de los que queremos aprender podremos ver una gr谩fica de la funci贸n de p茅rdida en cada 茅poca de entrenamiento y una animaci贸n de como se transforma el ruido inicial gaussiano en la distribuci贸n que hayamos elegido. Con la opci贸n de dibujar uno puede darse cuenta de la flexibilidad de estos m茅todos para aprender distribuciones arbitrarias.

## Instalaci贸n.
1. Descarga el repositorio.
2. Crea un entorno virtual en el repositorio con el comando `python3 -m venv .venv`. Aseg煤rate de estar en la carpeta principal del proyecto (donde est谩 `app.py`).
3. Activa el entorno `source .venv/bin/activate`.
4. Instala las librer铆as: `pip install -r requirements.txt`.
5. Ejecuta la aplicaci贸n `streamlit run app.py`.
   
## Teor铆a
El _Flow Matching_ es una t茅cnica de __IA generativa__ basada en ecuaciones diferenciales ordinarias que transportan distribuciones, desde una gaussiana normalmente --aunque no exclusivamente-- hasta la distribuci贸n te贸rica que subyace a una muestra de datos (o _dataset_). 

Rigurosamente el problema de generaci贸n se puede plantear de la siguiente manera:

> Dada una muestra de datos $X\rightsquigarrow p_{data}$ (texto, imagen o cualquier dato que pueda representarse de forma vectorial) queremos generar un nuevo punto que provenga de esta distribuci贸n.

La principal limitaci贸n es que no conocemos de qu茅 distribuci贸n provienen los datos. En _Flow Matching_ el objetivo es descubrir __qu茅 campo vectorial transporta la distribuci贸n inicial a la distribuci贸n de los datos__. Este campo se conoce en general:

> Se puede demostrar que si $X_0\rightsquigarrow p_{init}$ es una nube de puntos distribuida como $p_{init}$, entonces la nube de puntos soluci贸n del problema
> 
> 
> $\frac{dX_t}{dt}=u_t(X_t)$
>
> donde $\displaystyle \int u_t(x|z)\frac{p_t(x|z)p_{data}(z)}{p_t(x)} dz$, verifica que $X_1\rightsquigarrow p_{data}$. Entonces al final de la simulaci贸n ($t=1$) obtenemos una nube de puntos con la misma distribuci贸n de los datos.


Los t茅rminos que aparecen en la integral son t茅cnicos y no los explicar茅 aqu铆, pero observamos que para poder evaluar el campo necesitamos conocer la distribuci贸n $p_{data}$ (!!!). Ese era nuestro objetivo desde el principio. Es por eso que no podemos usar esta f贸rmula tal cual esta as铆 (incluso si la conoci茅ramos, esa integral es intratable).

En la pr谩ctica lo que hacemos es sustituir el campo vectorial (la integral intratable) por una red neuronal $u_t^\theta(x)$ y entrenarla para que reproduzca de forma fiel el valor de la integral. Es por eso que este es un m茅todo de ___Deep Learning___.

Para entrenar esta red lo que debemos de hacer es encontrar $\theta\in\mathbb{R}^N$ que minimice el error cuadr谩tico medio:

$\mathbb{E}[\Vert u_t^\theta(x)-u_t(x)\Vert^2]$, donde $x \rightsquigarrow p_{data}$

De nuevo, por no conocer el valor exacto de $u_t(x)$ recurrimos a otra funci贸n de p茅rdida: la _funci贸n de p茅rdida condicional_.

> $\mathbb{E}[\Vert u_t^\theta(x)-u_t(x|z)\Vert^2]$, donde $t \rightsquigarrow U[0,1],\quad z \rightsquigarrow p_{data},\quad x\rightsquigarrow p_t(路|z)$

Esta funci贸n s铆 es calculable (aunque no hayamos explicado qu茅 es $u_t(x|z)$ y adem谩s se demuestra que los par谩metros $\theta$ que minimizan la funci贸n de p茅rdida condicional minimizan tambi茅n la funci贸n de p茅rdida original que nos interesaba. As铆 que a todos los efectos el entrenamiento se puede producir usando la funci贸n de p茅rdida original. A esto se le conoce como

Una buena noticia es que la expresi贸n de la funci贸n de p茅rdida condicional en el caso en el que $p_{init}=\mathcal{N}(0,I)$ es una normal de media 0 y matriz de covarianza $I$, la identidad, es muy sencilla:

$\mathcal{L}(\theta)=\mathbb{E}[\Vert u_t^\theta(tz+(1-t)\varepsilon)-(z-\varepsilon)\Vert^2]$, donde $t\rightsquigarrow U[0,1],\quad z\rightsquigarrow p_{data}, \quad \varepsilon \rightsquigarrow \mathcal{N}(0,I)$

En resumen lo que debemos hacer es:

1. Tomar al azar $z$ del _dataset_.
2. Generar un n煤mero aleatorio $t \rightsquigarrow U[0,1]$.
3. Generar ruido $\varepsilon \rightsquigarrow \mathcal{N}(0,I)$.
4. Calcular $x=tz+(1-t)\varepsilon$.
5. Calcular la funci贸n de p茅rdida condicional: $\mathcal{L}(\theta)=\mathbb{E}[\Vert u_t^\theta(tz+(1-t)\varepsilon)-(z-\varepsilon)\Vert^2]$
6. Actualizar los par谩metros $\theta$ v铆a descenso de gradiente aplicado a $\mathcal{L}(\theta)$.


Una vez entrenado el modelo debemos integrar la ecuaci贸n con alg煤n m茅todo (Euler, RK4, etc.) y obtendremos en $t=1$ el dato generado.
